{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0b9d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "class GreenEarthNetGenerator:\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.files = sorted(glob.glob(os.path.join(data_dir, \"**/*.nc\"), recursive=True))\n",
    "        self.input_days = 50\n",
    "        self.target_days = 100\n",
    "        self.s2_bands = ['s2_B02', 's2_B03', 's2_B04', 's2_B8A']\n",
    "        self.eobs_vars = ['eobs_tg', 'eobs_fg', 'eobs_hu', 'eobs_pp', 'eobs_qq', 'eobs_rr', 'eobs_tn', 'eobs_tx']\n",
    "        \n",
    "        # Normalization stats from en21x_data.py\n",
    "        self.eobs_stats = {\n",
    "            'eobs_tg': {'mean': 8.9066, 'std': 9.7562},\n",
    "            'eobs_fg': {'mean': 2.7329, 'std': 1.4870},\n",
    "            'eobs_hu': {'mean': 77.5444, 'std': 13.5114},\n",
    "            'eobs_pp': {'mean': 1014.3310, 'std': 10.2626},\n",
    "            'eobs_qq': {'mean': 126.4792, 'std': 97.0552},\n",
    "            'eobs_rr': {'mean': 1.7713, 'std': 4.1480},\n",
    "            'eobs_tn': {'mean': 4.7707, 'std': 9.0450},\n",
    "            'eobs_tx': {'mean': 13.5680, 'std': 11.0820}\n",
    "        }\n",
    "        \n",
    "        self.dem_vars = ['alos_dem', 'cop_dem', 'nasa_dem']\n",
    "        \n",
    "    def compute_ndvi(self, red, nir):\n",
    "        # NDVI = (NIR - Red) / (NIR + Red)\n",
    "        denominator = nir + red\n",
    "        ndvi = np.divide(nir - red, denominator, out=np.zeros_like(denominator), where=denominator!=0)\n",
    "        # Normalize to [0, 1] as per methodology: [-1, 1] -> [0, 1]\n",
    "        ndvi_norm = (ndvi + 1) / 2\n",
    "        return np.clip(ndvi_norm, 0, 1)\n",
    "\n",
    "    def normalize_band(self, band_data):\n",
    "        # Simple min-max normalization or percentile based?\n",
    "        # The previous code used percentile. Let's stick to a robust normalization or just raw for now if not specified.\n",
    "        # User request didn't specify normalization, but usually it's good practice.\n",
    "        # However, for a generator \"to be used later\", raw values might be preferred unless specified.\n",
    "        # But previous dataset.py had normalization.\n",
    "        # Let's keep it simple: Raw values for now, or maybe 0-1 if we know the range.\n",
    "        # Sentinel-2 is usually 0-10000.\n",
    "        return np.clip(band_data / 10000.0, 0, 1)\n",
    "\n",
    "    def __call__(self):\n",
    "        for file_path in self.files:\n",
    "            try:\n",
    "                with xr.open_dataset(file_path) as ds:\n",
    "                    # Check if we have enough time steps\n",
    "                    if len(ds.time) < self.input_days + self.target_days:\n",
    "                        continue\n",
    "\n",
    "                    # --- Inputs (First 50 days) ---\n",
    "                    input_slice = slice(0, self.input_days)\n",
    "                    \n",
    "                    # 1. Sentinel-2 Bands\n",
    "                    s2_data = []\n",
    "                    for band in self.s2_bands:\n",
    "                        b_data = ds[band].isel(time=input_slice).values\n",
    "                        # Normalize? Let's do simple division by 10000 for S2\n",
    "                        s2_data.append(np.clip(b_data / 10000.0, 0, 1))\n",
    "                    \n",
    "                    # (50, 128, 128, 4)\n",
    "                    sentinel2 = np.stack(s2_data, axis=-1)\n",
    "                    \n",
    "                    # Check for NaNs in Sentinel-2\n",
    "                    # If any band is NaN, the pixel is missing/invalid\n",
    "                    s2_nans = np.isnan(sentinel2).any(axis=-1, keepdims=True) # (50, 128, 128, 1)\n",
    "                    \n",
    "                    # Replace NaNs with 0.0\n",
    "                    sentinel2 = np.nan_to_num(sentinel2, nan=0.0)\n",
    "\n",
    "                    # 2. Cloud Mask\n",
    "                    # mask > 0 means cloud/shadow etc.\n",
    "                    mask = ds['s2_mask'].isel(time=input_slice).values\n",
    "                    s2_mask = (mask > 0).astype(np.float32)\n",
    "                    s2_mask = np.expand_dims(s2_mask, axis=-1) # (50, 128, 128, 1)\n",
    "                    \n",
    "                    # Update mask to include NaNs (missing data)\n",
    "                    # If s2_nans is True, s2_mask should be 1 (masked)\n",
    "                    s2_mask = np.maximum(s2_mask, s2_nans.astype(np.float32))\n",
    "                \n",
    "                    # 3. Weather (E-OBS)\n",
    "                    weather_data = []\n",
    "                    for var in self.eobs_vars:\n",
    "                        w_data = ds[var].isel(time=input_slice).values\n",
    "                        # Normalize\n",
    "                        stats = self.eobs_stats[var]\n",
    "                        w_data = (w_data - stats['mean']) / stats['std']\n",
    "                        weather_data.append(w_data)                    \n",
    "                    \n",
    "                    # (50, 8)\n",
    "                    weather = np.stack(weather_data, axis=-1)\n",
    "                    # Handle NaNs in weather if any (though usually E-OBS is complete or interpolated)\n",
    "                    weather = np.nan_to_num(weather, nan=0.0)\n",
    "\n",
    "                    # 4. DEM\n",
    "                    dem_data = []\n",
    "                    for var in self.dem_vars:\n",
    "                        d_data = ds[var].values # (lat, lon)\n",
    "                        # Normalize by dividing by 500\n",
    "                        d_data = d_data / 500.0\n",
    "                        dem_data.append(d_data)\n",
    "                    \n",
    "                    # (128, 128, 3)\n",
    "                    dem = np.stack(dem_data, axis=-1)\n",
    "                    # Handle NaNs in DEM\n",
    "                    dem = np.nan_to_num(dem, nan=0.0)\n",
    "\n",
    "                    # 5. Geomorphology\n",
    "                    geom = ds['geom_cls'].values # (lat, lon)\n",
    "                    geomorphology = np.expand_dims(geom, axis=-1) # (128, 128, 1)\n",
    "                    geomorphology = np.nan_to_num(geomorphology, nan=0.0)\n",
    "                    \n",
    "                    # 6. Landcover (ESA WorldCover)\n",
    "                    lc = ds['esawc_lc'].values # (128, 128)\n",
    "                    \n",
    "                    # 7. Time\n",
    "                    times = ds.time.isel(time=input_slice).values\n",
    "                    ts = pd.to_datetime(times)\n",
    "                    \n",
    "                    # Cyclical features for Day of Year\n",
    "                    doy = ts.dayofyear\n",
    "                    doy_sin = np.sin(2 * np.pi * doy / 366.0)\n",
    "                    doy_cos = np.cos(2 * np.pi * doy / 366.0)\n",
    "                    \n",
    "                    # Normalize Year (approximate, assuming data is recent)\n",
    "                    # Let's map 2017-2021 to roughly [-1, 1] or [0, 1]\n",
    "                    # 2017 is start, 2021 is end.\n",
    "                    year_norm = (ts.year - 2019) / 2.0 \n",
    "                    \n",
    "                    time_feats = np.stack([year_norm, doy_sin, doy_cos], axis=-1).astype(np.float32) # (50, 3)\n",
    "\n",
    "                    x = {\n",
    "                        'sentinel2': sentinel2.astype(np.float32),\n",
    "                        's2_mask': s2_mask.astype(np.float32),\n",
    "                        'weather': weather.astype(np.float32),\n",
    "                        'dem': dem.astype(np.float32),\n",
    "                        'geomorphology': geomorphology.astype(np.float32),\n",
    "                        'time': time_feats\n",
    "                    }\n",
    "\n",
    "                    # --- Targets (Next 100 days) ---\n",
    "                    target_slice = slice(self.input_days+4, self.input_days + self.target_days, 5)\n",
    "                    \n",
    "                    red_t = ds['s2_B04'].isel(time=target_slice).values\n",
    "                    nir_t = ds['s2_B8A'].isel(time=target_slice).values\n",
    "                    \n",
    "                    ndvi_t = self.compute_ndvi(red_t, nir_t)\n",
    "                    \n",
    "                    # Apply cloud mask\n",
    "                    mask_t = ds['s2_mask'].isel(time=target_slice).values\n",
    "                    \n",
    "                    # Use np.where for safe broadcasting/masking\n",
    "                    ndvi_t = np.where(mask_t > 0, np.nan, ndvi_t)\n",
    "                        \n",
    "                    avail_t = ds['s2_avail'].isel(time=target_slice).values\n",
    "                    # avail_t is (time,)\n",
    "                    # Broadcast to (time, 128, 128)\n",
    "                    avail_t = avail_t[:, None, None]\n",
    "                    ndvi_t = np.where(avail_t == 0, np.nan, ndvi_t)\n",
    "\n",
    "                    # Prepare y with shape (20, 128, 128, 2)\n",
    "                    # Channel 0: NDVI\n",
    "                    # Channel 1: Landcover (repeated)\n",
    "                    \n",
    "                    # Expand NDVI to (20, 128, 128, 1)\n",
    "                    ndvi_t = np.expand_dims(ndvi_t, axis=-1)\n",
    "                    \n",
    "                    # Prepare Landcover (1, 128, 128, 1) -> (20, 128, 128, 1)\n",
    "                    lc_expanded = np.expand_dims(np.expand_dims(lc, axis=0), axis=-1)\n",
    "                    lc_t = np.tile(lc_expanded, (ndvi_t.shape[0], 1, 1, 1))\n",
    "                    \n",
    "                    y = np.concatenate([ndvi_t, lc_t], axis=-1).astype(np.float32)\n",
    "\n",
    "                    yield x, y\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "    def get_dataset(self) -> tf.data.Dataset:\n",
    "        # Define output signature\n",
    "        output_signature = (\n",
    "            {\n",
    "                'sentinel2': tf.TensorSpec(shape=(50, 128, 128, 4), dtype=tf.float32),\n",
    "                's2_mask': tf.TensorSpec(shape=(50, 128, 128, 1), dtype=tf.float32),\n",
    "                'weather': tf.TensorSpec(shape=(50, 8), dtype=tf.float32),\n",
    "                'dem': tf.TensorSpec(shape=(128, 128, 3), dtype=tf.float32),\n",
    "                'geomorphology': tf.TensorSpec(shape=(128, 128, 1), dtype=tf.float32),\n",
    "                'time': tf.TensorSpec(shape=(50, 3), dtype=tf.float32)\n",
    "            },\n",
    "            tf.TensorSpec(shape=(20, 128, 128, 2), dtype=tf.float32)\n",
    "        )\n",
    "        \n",
    "        return tf.data.Dataset.from_generator(\n",
    "            self.__call__,\n",
    "            output_signature=output_signature\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441bedf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"ConvFormer\")\n",
    "class QuantileLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, quantiles, name='quantile_loss', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.quantiles = quantiles\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # y_true: (B, 20, 128, 128, 2) - Channel 0 is NDVI\n",
    "        # y_pred: (B, NumQuantiles, 20, 128, 128, 1)\n",
    "        \n",
    "        y_true_ndvi = y_true[..., 0:1] # (B, 20, 128, 128, 1)\n",
    "        \n",
    "        # Expand y_true to match y_pred quantiles\n",
    "        # (B, 1, 20, 128, 128, 1)\n",
    "        y_true_exp = tf.expand_dims(y_true_ndvi, axis=1)\n",
    "        \n",
    "        # Mask NaNs\n",
    "        mask = tf.logical_not(tf.math.is_nan(y_true_exp))\n",
    "        y_true_safe = tf.where(mask, y_true_exp, 0.0)\n",
    "        \n",
    "        loss = 0.0\n",
    "        for i, q in enumerate(self.quantiles):\n",
    "            q_pred = y_pred[:, i:i+1, ...] # (B, 1, 20, 128, 128, 1)\n",
    "            error = y_true_safe - q_pred\n",
    "            q_loss = tf.maximum(q * error, (q - 1) * error)\n",
    "            loss += q_loss\n",
    "            \n",
    "        # Apply mask\n",
    "        loss = tf.where(mask, loss, 0.0)\n",
    "        \n",
    "        # Average over valid pixels\n",
    "        num_valid = tf.maximum(tf.reduce_sum(tf.cast(mask, tf.float32)), 1.0)\n",
    "        return tf.reduce_sum(loss) / num_valid\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'quantiles': self.quantiles})\n",
    "        return config\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"ConvFormer\")\n",
    "class VegetationScoreLoss(tf.keras.losses.Loss):\n",
    "    \"\"\"\n",
    "    Vegetation Score Loss.\n",
    "    \n",
    "    Maximizes the Vegetation Score:\n",
    "    VegScore = 2 - 1/mean(NNSE_veg)\n",
    "    where NNSE = 1 / (2 - NSE)\n",
    "    and NSE is Nash-Sutcliffe Efficiency on cloud-free vegetation pixels.\n",
    "    \n",
    "    Loss = 1 - VegScore (to minimize)\n",
    "         = 1 - (2 - 1/mean(NNSE))\n",
    "         = 1/mean(NNSE) - 1\n",
    "    \n",
    "    Vegetation classes (ESA WorldCover):\n",
    "    10: Tree cover\n",
    "    20: Shrubland\n",
    "    30: Grassland\n",
    "    \"\"\"\n",
    "    def __init__(self, name='vegetation_score_loss', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        \n",
    "    def call(self, y_true, y_pred):\n",
    "        # y_true: (batch, 20, 128, 128, 2) [NDVI, Landcover]\n",
    "        # y_pred: (batch, 20, 128, 128, 1)\n",
    "        \n",
    "        y_true_ndvi = y_true[..., 0:1]\n",
    "        landcover = y_true[..., 1:2]\n",
    "        \n",
    "        # Mask for valid pixels (not NaN)\n",
    "        valid_mask = tf.logical_not(tf.math.is_nan(y_true_ndvi))\n",
    "        \n",
    "        # Mask for vegetation pixels (10, 20, 30)\n",
    "        # ESA WorldCover: 10=Trees, 20=Shrubland, 30=Grassland\n",
    "        veg_mask = (landcover == 10) | (landcover == 20) | (landcover == 30)\n",
    "        \n",
    "        # Combined mask: Valid AND Vegetation\n",
    "        mask = valid_mask & veg_mask\n",
    "        \n",
    "        # Replace NaNs with zeros for calculation\n",
    "        y_true_safe = tf.where(valid_mask, y_true_ndvi, 0.0)\n",
    "        \n",
    "        # Count valid observations per pixel\n",
    "        valid_count_per_pixel = tf.reduce_sum(tf.cast(valid_mask, tf.float32), axis=1, keepdims=True) # (B, 1, 128, 128, 1)\n",
    "        \n",
    "        # Sum of true values\n",
    "        sum_true = tf.reduce_sum(y_true_safe, axis=1, keepdims=True)\n",
    "        mean_true = tf.math.divide_no_nan(sum_true, valid_count_per_pixel)\n",
    "        \n",
    "        # Numerator: Sum of squared errors\n",
    "        sse = tf.reduce_sum(tf.square(y_true_safe - y_pred) * tf.cast(valid_mask, tf.float32), axis=1, keepdims=True)\n",
    "        \n",
    "        # Denominator: Sum of squared deviations from mean\n",
    "        sst = tf.reduce_sum(tf.square(y_true_safe - mean_true) * tf.cast(valid_mask, tf.float32), axis=1, keepdims=True)\n",
    "        \n",
    "        # NSE = 1 - SSE/SST\n",
    "        epsilon = 1e-6\n",
    "        nse = 1.0 - (sse / (sst + epsilon))\n",
    "        \n",
    "        # NNSE = 1 / (2 - NSE)\n",
    "        nnse = 1.0 / (2.0 - nse)\n",
    "        \n",
    "        # Now average NNSE over vegetation pixels\n",
    "        # veg_mask is (B, 20, 128, 128, 1). We need a spatial mask (B, 1, 128, 128, 1)\n",
    "        # A pixel is vegetation if it is vegetation at any time step (it's static)\n",
    "        spatial_veg_mask = veg_mask[:, 0:1, :, :, :] # (B, 1, 128, 128, 1)\n",
    "        \n",
    "        masked_nnse = nnse * tf.cast(spatial_veg_mask, tf.float32)\n",
    "        \n",
    "        sum_nnse = tf.reduce_sum(masked_nnse)\n",
    "        count_veg = tf.reduce_sum(tf.cast(spatial_veg_mask, tf.float32))\n",
    "        \n",
    "        mean_nnse = tf.math.divide_no_nan(sum_nnse, count_veg)\n",
    "        \n",
    "        # Loss = 1/mean_nnse - 1\n",
    "        loss = tf.where(\n",
    "            count_veg > 0,\n",
    "            (1.0 / (mean_nnse + epsilon)) - 1.0,\n",
    "            0.0\n",
    "        )\n",
    "        \n",
    "        return loss\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"ConvFormer\")\n",
    "class CombinedLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, quantiles, veg_weight=0.1, name='combined_loss', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.quantiles = quantiles\n",
    "        self.veg_weight = veg_weight\n",
    "        self.quantile_loss = QuantileLoss(quantiles)\n",
    "        self.veg_loss = VegetationScoreLoss()\n",
    "        \n",
    "    def call(self, y_true, y_pred):\n",
    "        # y_true: (B, 20, 128, 128, 2)\n",
    "        # y_pred: (B, NumQuantiles, 20, 128, 128, 1)\n",
    "        \n",
    "        # 1. Quantile Loss\n",
    "        q_loss = self.quantile_loss(y_true, y_pred)\n",
    "        \n",
    "        # 2. Vegetation Score Loss\n",
    "        # Use the median prediction (usually the middle quantile) for the vegetation score\n",
    "        # Assuming quantiles are sorted, median is at index len(quantiles)//2\n",
    "        median_idx = len(self.quantiles) // 2\n",
    "        \n",
    "        # Extract median prediction: (B, NumQuantiles, 20, 128, 128, 1) -> (B, 20, 128, 128, 1)\n",
    "        # Slicing with integer index removes the dimension\n",
    "        y_pred_median = y_pred[:, median_idx, ...]\n",
    "        \n",
    "        v_loss = self.veg_loss(y_true, y_pred_median)\n",
    "        \n",
    "        return q_loss + self.veg_weight * v_loss\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'quantiles': self.quantiles,\n",
    "            'veg_weight': self.veg_weight\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"ConvFormer\")\n",
    "class QuantileRegressionHead(layers.Layer):\n",
    "    def __init__(self, forecast_horizon, quantiles, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.quantiles = quantiles\n",
    "        self.num_quantiles = len(quantiles)\n",
    "        \n",
    "        self.quantile_heads = [\n",
    "            tf.keras.Sequential([\n",
    "                layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "                layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "                layers.Conv2D(forecast_horizon, 1) \n",
    "            ], name=f'q_head_{i}')\n",
    "            for i in range(self.num_quantiles)\n",
    "        ]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        outputs = []\n",
    "        for head in self.quantile_heads:\n",
    "            pred = head(inputs)\n",
    "            pred = tf.transpose(pred, perm=[0, 3, 1, 2]) # (B, 20, 128, 128)\n",
    "            pred = tf.expand_dims(pred, axis=-1) # (B, 20, 128, 128, 1)\n",
    "            outputs.append(pred)\n",
    "            \n",
    "        return tf.stack(outputs, axis=1)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"forecast_horizon\": self.forecast_horizon,\n",
    "            \"quantiles\": self.quantiles,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"ConvFormer\")\n",
    "class ConvFormer(tf.keras.Model):\n",
    "    def __init__(self, forecast_horizon=20, quantiles=[0.1, 0.5, 0.9], **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.quantiles = quantiles\n",
    "        \n",
    "        # --- Encoder ---\n",
    "        self.spatial_encoder = tf.keras.Sequential([\n",
    "            layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "            layers.MaxPooling2D(2), \n",
    "            layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "            layers.MaxPooling2D(2), \n",
    "            layers.Conv2D(128, 3, padding='same', activation='relu')\n",
    "        ], name='spatial_encoder')\n",
    "        \n",
    "        self.td_spatial = layers.TimeDistributed(self.spatial_encoder)\n",
    "        \n",
    "        self.conv_lstm = layers.ConvLSTM2D(\n",
    "            filters=128, kernel_size=3, padding='same', return_sequences=False\n",
    "        )\n",
    "        \n",
    "        # --- Fusion ---\n",
    "        self.weather_mlp = tf.keras.Sequential([\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(128, activation='relu')\n",
    "        ], name='weather_mlp')\n",
    "        \n",
    "        self.dem_encoder = tf.keras.Sequential([\n",
    "            layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "            layers.MaxPooling2D(2), \n",
    "            layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "            layers.MaxPooling2D(2), \n",
    "            layers.Conv2D(128, 3, padding='same', activation='relu')\n",
    "        ], name='dem_encoder')\n",
    "        \n",
    "        self.fusion_conv = layers.Conv2D(256, 3, padding='same', activation='relu')\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.UpSampling2D(2), \n",
    "            layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "            layers.UpSampling2D(2), \n",
    "            layers.Conv2D(64, 3, padding='same', activation='relu')\n",
    "        ], name='decoder')\n",
    "        \n",
    "        # --- Head ---\n",
    "        self.head = QuantileRegressionHead(forecast_horizon, quantiles)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if isinstance(inputs, dict):\n",
    "            sentinel2 = inputs['sentinel2']\n",
    "            weather = inputs['weather']\n",
    "            dem = inputs['dem']\n",
    "        else:\n",
    "            sentinel2, weather, dem = inputs\n",
    "            \n",
    "        x_spatial = self.td_spatial(sentinel2)\n",
    "        x_temporal = self.conv_lstm(x_spatial)\n",
    "        \n",
    "        w_feat = tf.reduce_mean(weather, axis=1) \n",
    "        w_emb = self.weather_mlp(w_feat) \n",
    "        w_emb = tf.reshape(w_emb, (-1, 1, 1, 128))\n",
    "        w_emb = tf.tile(w_emb, [1, 32, 32, 1]) \n",
    "        \n",
    "        d_emb = self.dem_encoder(dem)\n",
    "        \n",
    "        fused = layers.concatenate([x_temporal, w_emb, d_emb], axis=-1)\n",
    "        fused = self.fusion_conv(fused) \n",
    "        \n",
    "        decoded = self.decoder(fused)\n",
    "        \n",
    "        outputs = self.head(decoded)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"forecast_horizon\": self.forecast_horizon,\n",
    "            \"quantiles\": self.quantiles,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d5f48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "\n",
    "def train_convformer(\n",
    "    train_dir,\n",
    "    val_dir=None,\n",
    "    batch_size=4,\n",
    "    epochs=200,\n",
    "    learning_rate=1e-4,\n",
    "    checkpoint_dir='checkpoints',\n",
    "    log_dir='logs/convformer',\n",
    "    quantiles=[0.1, 0.5, 0.9]\n",
    "):\n",
    "    # Create directories\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"Loading data from {train_dir}...\")\n",
    "    \n",
    "    # 1. Dataset\n",
    "    generator = GreenEarthNetGenerator(train_dir)\n",
    "    train_dataset = generator.get_dataset()\n",
    "    \n",
    "    # Shuffle and Batch\n",
    "    train_dataset = train_dataset.shuffle(100).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    val_dataset = None\n",
    "    if val_dir and os.path.exists(val_dir):\n",
    "        print(f\"Loading validation data from {val_dir}...\")\n",
    "        val_generator = GreenEarthNetGenerator(val_dir)\n",
    "        val_dataset = val_generator.get_dataset()\n",
    "        val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    # 2. Model\n",
    "    print(\"Creating ConvFormer model...\")\n",
    "    model = ConvFormer(forecast_horizon=20, quantiles=quantiles)\n",
    "    \n",
    "    # Build model to print summary\n",
    "    # Create dummy input to build the model\n",
    "    dummy_input = {\n",
    "        'sentinel2': tf.zeros((1, 50, 128, 128, 4)),\n",
    "        's2_mask': tf.zeros((1, 50, 128, 128, 1)),\n",
    "        'weather': tf.zeros((1, 50, 8)),\n",
    "        'dem': tf.zeros((1, 128, 128, 3)),\n",
    "        'geomorphology': tf.zeros((1, 128, 128, 1)),\n",
    "        'time': tf.zeros((1, 50, 3))\n",
    "    }\n",
    "    model(dummy_input)\n",
    "    model.summary()\n",
    "    \n",
    "    # 3. Compile\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    loss = CombinedLoss(quantiles=quantiles, veg_weight=0.1)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss=loss)\n",
    "    \n",
    "    # 4. Callbacks\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, 'convformer_best.weights.h5')\n",
    "    \n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            checkpoint_path,\n",
    "            monitor='loss',\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "            mode='min',\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='loss',\n",
    "            factor=0.5,\n",
    "            patience=10,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1,\n",
    "            mode='min'\n",
    "        ),\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='loss',\n",
    "            patience=25,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1,\n",
    "            mode='min'\n",
    "        ),\n",
    "        tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=log_dir,\n",
    "            histogram_freq=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # 5. Train\n",
    "    print(\"Starting training...\")\n",
    "    try:\n",
    "        history = model.fit(\n",
    "            train_dataset,\n",
    "            validation_data=val_dataset,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training interrupted.\")\n",
    "        return model\n",
    "    \n",
    "    # Save final weights\n",
    "    final_weights_path = os.path.join(checkpoint_dir, 'convformer_final.weights.h5')\n",
    "    print(f\"Saving final weights to {final_weights_path}...\")\n",
    "    model.save_weights(final_weights_path)\n",
    "    \n",
    "    print(\"Training complete.\")\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='Train ConvFormer model')\n",
    "    parser.add_argument(\n",
    "        '--train-dir',\n",
    "        type=str,\n",
    "        default='/home/me/workspace/probformer/data/greenearthnet/traint_test',\n",
    "        help='Directory containing training .nc files'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--val-dir',\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help='Directory containing validation .nc files'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--batch-size',\n",
    "        type=int,\n",
    "        default=4,\n",
    "        help='Batch size for training'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--epochs',\n",
    "        type=int,\n",
    "        default=1000,\n",
    "        help='Maximum number of epochs'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--learning-rate',\n",
    "        type=float,\n",
    "        default=1e-3,\n",
    "        help='Initial learning rate'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--checkpoint-dir',\n",
    "        type=str,\n",
    "        default='checkpoints',\n",
    "        help='Directory to save model checkpoints'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--log-dir',\n",
    "        type=str,\n",
    "        default='logs/convformer',\n",
    "        help='Directory for TensorBoard logs'\n",
    "    )\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Use env var if argument is default and env var is set? \n",
    "    # Or just rely on args. The user specified the path in the prompt.\n",
    "    \n",
    "    train_convformer(\n",
    "        train_dir=args.train_dir,\n",
    "        val_dir=args.val_dir,\n",
    "        batch_size=args.batch_size,\n",
    "        epochs=args.epochs,\n",
    "        learning_rate=args.learning_rate,\n",
    "        checkpoint_dir=args.checkpoint_dir,\n",
    "        log_dir=args.log_dir\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
